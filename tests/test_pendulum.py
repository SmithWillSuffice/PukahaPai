#~/usr/bin/env python3
'''
Unit test for the pendulum ODEs.
We only check the cmdl version.

Passed 2025-07-26
```
PukahaPai$ pytest -k Pendulum     # case sensitive
================================================= test session starts ==================================================
platform linux -- Python 3.10.12, pytest-8.0.0, pluggy-1.4.0
rootdir: /home/geon/dev/python/economics/PukahaPai
plugins: anyio-4.2.0
collected 4 items                                                                                                      

tests/test_pendulum.py ....                                                                                      [100%]

================================================== 4 passed in 7.96s ===================================================
```

| Copyright Â© 2025, Bijou M. Smith
| License: GNU General Public License v3.0  <https://www.gnu.org/licenses/gpl-3.0.html>
'''
import pytest
import subprocess
import csv
from pathlib import Path

model_name = 'pendulum'
project_root = Path.cwd()
code_gen = "./generate_julia_odesolver.py"
n_rows = 5   # number of rows to output and check

class TestPendulumModel:
    """Test suite for generate_julia_odesolver.py script and jinja template."""
        
    @pytest.fixture(scope="class", autouse=True)
    def build_and_run_simulation(self):
        """Fixture to build the Julia model and run the simulation before tests."""
        # Step 1: Generate Julia code
        gen_result = subprocess.run(
            [code_gen, model_name],
            capture_output=True, text=True, timeout=30, cwd=project_root
        )
        assert gen_result.returncode == 0, f"Script failed: {gen_result.stderr}"

        output_file = project_root / "models" / f"{model_name}_cmdl.jl"
        assert output_file.exists(), "Generated Julia file not found"

        # Step 2: Run the Julia simulation
        csv_output = project_root / "models" / f"{model_name}.csv"
        if csv_output.exists():
            csv_output.unlink()

        run_result = subprocess.run(
            ["julia", str(output_file)],
            capture_output=True, text=True, timeout=60, cwd=project_root
        )
        assert run_result.returncode == 0, f"Julia simulation failed: {run_result.stderr}"
        assert csv_output.exists(), "CSV output file not generated"


    @pytest.fixture(scope="class")
    def expected_output_values(self):
        """Expected values for the first 10 time steps of simulation
        These should be updated based on your known good output."""
        # return [
        #     [0.01,0.7850512998056338,-0.06932447565427709],
        #     [0.02,0.784011901744952,-0.13853157488117282],
        #     [0.03,0.782281219517248,-0.2075733101370855],  
        #     [0.04,0.7798611468217939,-0.27640165819464124],
        #     [0.05,0.7767540580345769,-0.3449684965228204],
        #     ]
        return [ [0.00015625,0.7853978306492473,-0.0010838448173090063],
            [0.0003125,0.7853974919504453,-0.002167672332710101],
            [0.00046875,0.7853969839063261,-0.003251482362929191],
            [0.000625,0.7853963065196503,-0.00433527472469776],
            [0.00078125,0.7853954597932074,-0.005419049234752805]
             ]

    
    def test_generate_julia_cmdl(self):
        """Test generating the  command-line Julia script."""
        result = subprocess.run([ code_gen, model_name
            ], capture_output=True, text=True, timeout=30, cwd=project_root)
            
        assert result.returncode == 0, f"Script failed with error: {result.stderr}"
            
        # Check that the output file was generated
        output_file = project_root / "models" / f"{model_name}_cmdl.jl"
        assert output_file.exists(), "Generated Julia file not found"
    

    def test_run_julia_simulation(self):
        """Test running the generated Julia simulation"""
        julia_script = project_root / "models" / f"{model_name}_cmdl.jl"
        csv_output = project_root / "models" / f"{model_name}.csv"
        
        # Ensure the Julia script exists (should be generated by previous test)
        assert julia_script.exists(), "Julia script not found. Run generation test first."
        
        # Remove existing CSV if it exists to ensure clean test
        if csv_output.exists():
            csv_output.unlink()
        
        # Run the Julia simulation
        result = subprocess.run([
            'julia', str(julia_script)
        ], capture_output=True, text=True, timeout=60, cwd=project_root)
        
        assert result.returncode == 0, f"Julia simulation failed: {result.stderr}"
        
        # Check that CSV output was generated
        assert csv_output.exists(), "CSV output file not generated"
    

    def test_csv_output_format(self):
        """Test that the CSV output has the correct format"""
        csv_output = project_root / "models" / f"{model_name}.csv"
        assert csv_output.exists(), "CSV output file not found"
        with open(csv_output, 'r') as f:
            reader = csv.reader(f)
            rows = list(reader)
        # Check that we have data
        assert len(rows) > n_rows, "CSV should have at least 10 rows of data"
        # Check that each row has the expected number of columns (adjust as needed)
        # Assuming format: time, theta, omega (3 columns)
        for i, row in enumerate(rows[1:n_rows+1]):
            assert len(row) >= 3, f"Row {i} should have at least 3 columns, got {len(row)}"
            assert  [float(val) for val in row[:3]], f"Row {i} contains non-numeric values: {row[:3]}"
    
    

    def test_csv_output_values(self, expected_output_values):
        """Test that the CSV output values are within expected tolerances"""
        csv_output = project_root / "models" / f"{model_name}.csv"
        assert csv_output.exists(), "CSV output file not found"
        
        with open(csv_output, 'r') as f:
            reader = csv.reader(f)
            rows = list(reader)
        
        # Test first 10 rows against expected values
        tolerance = 1e-6  # Adjust tolerance as needed
        
        for i, (actual_row, expected_row) in enumerate(zip(rows[1:n_rows+1], expected_output_values)):
            for j, (actual_val, expected_val) in enumerate(zip(actual_row[:n_rows], expected_row)):
                actual_float = float(actual_val)
                expected_float = float(expected_val)
                
                assert abs(actual_float - expected_float) <= tolerance, \
                    f"Row {i}, Column {j}: Expected {expected_float}, got {actual_float} (tolerance: {tolerance})"
    

if __name__ == "__main__":
    # Allow running the test directly
    pytest.main([__file__, "-v"])